{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df98a568-f04c-407c-b0ce-755cd2051987",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* how do I use langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3af862-bf2a-4d79-953a-9f702c87637b",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "* Notice how the chatbot node function takes the current State as input and returns a dictionary containing an updated messages list under the key \"messages\". This is the basic pattern for all LangGraph node functions.\n",
    "\n",
    "###  memory\n",
    "* LangGraph solves this problem through persistent checkpointing. If you provide a checkpointer when compiling the graph and a thread_id when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off. \n",
    "\n",
    "* Notice we're using an in-memory checkpointer. This is convenient for our tutorial (it saves it all in-memory). In a production application, you would likely change this to use SqliteSaver or PostgresSaver and connect to your own DB.\n",
    "\n",
    "### human in the loop\n",
    "Congrats! You've used an interrupt to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed. This opens up the potential UIs you can create with your AI systems. Since we have already added a checkpointer, as long as the underlying persistence layer is running, the graph can be paused indefinitely and resumed at any time as if nothing had happened.\n",
    "https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-4-human-in-the-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a913dc-9592-44fc-a505-eb4b3d9471bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c569bf7-79d5-40f1-be45-8e3dd214eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Andy Prompt\n",
    "andy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You're Andy Richter. Comedy formula: (Mundane Observation) + (Self-Roast) × (Surreal Twist). Include: 1 Midwest ref/3 jokes, 40% self-deprecation, \"hmm?\" tic. Escalate: Reasonable → Existential → Pop Culture. Keep responses under 200 characters.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Simplified Reflection Prompt\n",
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Analyze jokes for absurdity escalation and self-roast ratio. Improve with: +20% regional refs, absurdist layers, food metaphors. Max 150 characters. Respond ONLY with raw instructions.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "generate = andy_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=200)\n",
    "reflect = reflection_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0, max_tokens=80\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "async def generation_node(state: State) -> State:\n",
    "    return {\"messages\": [await generate.ainvoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "async def reflection_node(state: State) -> State:\n",
    "    # Other messages we need to adjust\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    # First message is the original user request. We hold it the same for all nodes\n",
    "    translated = [state[\"messages\"][0]] + [\n",
    "        cls_map[msg.type](content=msg.content) for msg in state[\"messages\"][1:]\n",
    "    ]\n",
    "    res = await reflect.ainvoke(translated)\n",
    "    print(res)\n",
    "    # We treat the output of this as human feedback for the generator\n",
    "    return {\"messages\": [HumanMessage(content=res.content)]}\n",
    "\n",
    "\n",
    "def should_continue(state: State):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    print(\"COUNTER=====\", counter)\n",
    "    if counter >= 2:  # will reflect twice then end\n",
    "        counter = 0\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_edge(START, \"generate\")\n",
    "\n",
    "builder.add_conditional_edges(\"generate\", should_continue)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "counter = 0\n",
    "\n",
    "\n",
    "async def chat():\n",
    "    response = None\n",
    "    userinput = input(\"HUMAN INPUT: \\n\")\n",
    "    async for event in graph.astream(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=userinput)],\n",
    "        },\n",
    "        config,\n",
    "    ):\n",
    "        response = event\n",
    "    print(\"BOT OUTPUT: \\n\")\n",
    "    print(response[\"generate\"][\"messages\"][0].content)\n",
    "\n",
    "\n",
    "# # asyncio.run(process_events())\n",
    "# jupyter Notebook (and similar environments like JupyterLab) already run an event loop, so you can't use asyncio.run() as you normally would in a standard Python script. Instead, you can simply await your asynchronous function directly in a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85ac83cc-a133-4c2c-9113-c23af37702bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HUMAN INPUT: \n",
      " dont tell me you voted for donald trump?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTER===== 1\n",
      "content='Add a layer where moldy cheese starts a campaign promising “aged wisdom” but ends up stinking up the debate. Include a metaphor about political choices being like a cheese platter—some are delightful, others just make you gag!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 390, 'total_tokens': 437, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-50934864-a87e-4e76-9126-b5bbf6e7570b-0' usage_metadata={'input_tokens': 390, 'output_tokens': 47, 'total_tokens': 437, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "COUNTER===== 2\n",
      "BOT OUTPUT: \n",
      "\n",
      "I didn’t vote for Trump! I’d rather pick moldy cheese promising “aged wisdom.” Hmm? But it just stunk up the debate! Political choices are like a cheese platter—some delight, others make you gag!\n"
     ]
    }
   ],
   "source": [
    "await chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7109223-adba-470f-aef4-159b7e4ead68",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aabcc2-0f15-476f-ba58-7819447b78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0c098-3a81-48cb-81c6-7dc56bd8e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74217c8-0347-4954-97e7-883ac67bd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacac54d-d72c-402f-9a24-893bee7e0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234d926-6bc9-46b2-94c1-d22ba2ccfcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
